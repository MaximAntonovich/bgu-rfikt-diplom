\subsection{t-SNE}

\subsubsection{Обоснование алгоритма}

Алгоритм t-SNE преобразует многомерные данные $X$ в их двух- или трехмерное представление $Y$, которое может быть изображено на обычном графике. Целью такого преобразования является сохранение настолько точно, насколько возможно кластерной структуры исходного множества $X$ в представлении $Y$. Для этой цели существуют различные алгоритмы, в зависимости от структуры, которую они сохраняют. Традиционные линейные модели, такие как метод главных компонент и многомерное шкалирование, фокусируются на низкоразмерном представлении непохожих точек максимально далеко друг от друга. В случае, когда многомерные данные лежат на нелинейном многообразии более важным становится не разделение непохожих точек, а сохранение близости похожих в малоразмерном пространстве, что часто бывает невозможно с классическими линейными моделями.

t-SNE представляет собой алгоритм построения матрицы схожести исходных данных и её визуализации. t-SNE показал эффективное уменьшение размерности в самых разных областях. Отправной точкой алгоритма является собоставление попарных расстояний между точками и условных вероятностей, показывающих подобие точек. Более формально, подобию точек $x_i$ и $x_j$ соотвествует условная верятность $p_{i|j}$, которая рассчитывается как относительная вероятность выбрать $x_j$ среди остальных точек при оценки плотности нормальным распределением. 
\begin{equation}
	p_{i|j} = \frac{\exp(-||x_j - x_i||^2 / 2 \sigma_j^2)}{\sum_{k \neq j}\exp(-||x_j - x_k||^2 / 2 \sigma_j^2)}
\end{equation}
данная модель зависит от $\sigma_i$. Способ вычисления подходящего $\sigma_i$ будет изложен далее. Так как нас интересуют только попарные сходимости, $p_{i|i}$ мы можем задать равным 0. Для отображений $y_i$ и $y_j$ возможно задать схожую условную вероятность $q_{i|j}$. Её тоже можно оценить схожим образом, но для удобства задать $\sigma_i = \frac{1}{\sqrt{2}}$:
\begin{equation}
		q_{i|j} = \frac{\exp(-||y_j - y_i||^2)}{\sum_{k \neq j}\exp(-||y_j - y_k||^2)}
\end{equation}

Если соответствие $X$ и $Y$ корректно отображает локальную структуру исходных данных, $p_{i|j}$ и $q_{i|j}$ должны быть равны. Основываясь на этом утверждении, t-SNE минимизирует расхождение между $p_{i|j}$ и $q_{i|j}$. Естественной мерой расхождения между двумя распределениями, или что то же, приближения одного распределения другим является дивергенция Кульбака-Лейблера. SNE минимизирует сумму дивергенций Кульбака-Лейблера по всем парам объектов, используя метод градиентного спуска. Целевая функция задается следующим образом: 
\begin{equation}
	C = \sum_i KL(P_i | Q_i) = \sum_i \sum_j p_{j|i} \log{\frac{p_{j|i}}{q_{j|i}}}
\end{equation}
$P_i$ здесь это условное распределение точек исходного множества $X$, а $Q_i$ условное распределение точек образа $Y$. Из-за несимметричности дивергенции Кульбака-Лейблера различные типы ошибок в отображении имеют разный вес. Например сильное увеличение значения целевой функции вызывает использование далеко отстоящих точек отображения для рядом стоящих точек в исходных данных, т.е. маленькое $q_{i|j}$ при большом $p_{i|j}$, так как использование близких точек отображения для отстоящих точек в исходных данных вызывает только небольшой прирост. Подобный дисбаланс приводит к задаче отсеивания нерелевантных распределений $Q_i$, что можно сделать с помощью подбора подходящей дисперсии для гауссовой модели в исходных данных $\sigma_i$.

Так как исходные данные часто распределены неравномерно, нельзя ожидать, что существует единственное $\sigma_i$, аппроксимирующее $X$. В районах с более плотным скоплением точек $\sigma_i$ будет меньше, чем в разреженных областях. При этом каждое значение $\sigma_i$ порождает соотвествующее распределение $P_i$ по всем точкам $X$. Это распределение имеет энтропию, которая возрастает при росте $\sigma_i$. Для контроля значения $\sigma_i$ t-SNE вводит параметр размытости(perplexity) который задается пользователем. 
\begin{equation}
	Perp(P_i) = 2^{H(P_i)}
\end{equation}
$H(P_i)$ здесь это энтропия Шеннона:
\begin{equation}
	H(P_i) = - \sum_i p_{i|j} \log_2{p_{i|j}}
\end{equation}
Размытость интерпретируется как гладкая мера эффективного числа соседей. Результат t-SNE относительно устойчив к разным значениям размытости, типичные значения лежат в пределах от 5 до 50. $\sigma_i$ при этом подбирается бинарным поиском.

Минимизация целевой функции производится методом градиентного спуска. Градиент целевой функции имеет достаточно простую форму:
\begin{equation}
	\frac{\delta C}{\delta y_i} = 2 \sum_j (p_{j|i} - q_{j|i} + p_{i|j} - q_{i|j})(y_i - y_j)
\end{equation}
Интересно, что градиент допускает физическую интерпретацию: результирующую силу, созданную множеством пружин между точкой отображения $y_i$ и остальными точками отображения $y_j$. Все пружины испытывают деформацию в направлении $y_i - y_j$. При этом точки, лежащие между $y_i$ и $y_j$ притягиваются или отталкиваются, в зависимости от того лежат ли $y_i$ и $y_j$ ближе или дальше расстояния, соответствующего расстоянию между $x_i$ и $x_j$. $(p_{j|i} - q_{j|i} + p_{i|j} - q_{i|j})$, имеющее смысл расхождения между попарными сходствами, интерпретируется как жесткость пружины. 

Градиентный спуск инициализируется точками, выбранными из изотропного Гауссового распределения, центрированного вокруг оригинала. Чтобы ускорить сходимость и избежать схождения в неоптимальный локальный минимум, на ранних итерациях к градиенту добавляется относительно ,большая задержка. Другими словами, к текущему градиенту добавляется экспоненциально затухающая сумма градиентов на предыдущих шагах, что записывается следующим образом:
\begin{equation}
	\mathcal{Y}^{(t)} = \mathcal{Y}^{(t-1)} + \eta \frac{\delta C}{\delta \mathcal{Y}} + \alpha(t) (\mathcal{Y}^{(t-1)} - \mathcal{Y}^{(t-2)})
\end{equation}

Так же, для улучшения сходимости, на каждой итерации к точкам имеет смысл добавлять гауссовский шум. Постепенно уменьшая дисперсию шума можно создать эффект имитации отжига. t-SNE имеет тенденцию лучше аппроксимировать стуктуру данных при очень медленном изменении дисперсии шума. К сожалению, этот приём создаёт зависимость результата от исходной дисперсии шума и параметра снижения. Более того, эти параметры взаимодействуют с параметрами задержки градиента и могут создавать различные ложные корреляции в отбражении. Этого можно избежать, добавив в t-SNE методы подбора параметров, допускающие выпуклую оптимизацию, для которой разработаны эффективные алгоритмы, что сократит время работы алгоритма по сравнению с подбором параметров методом имитации отжига.

Так же существуют раазличные вариации алгоритма SNE, исправляющие его недостатки. Например, недостатком SNE называют несимметричность целевой функции:$p_{i|j} \neq p_{j|i}$. Если точка $x_i$ является выбросом, то её условное распределение будет вносить слабый вклад в целевую функцию, что приводит к неопределённости положения её образа $y_i$ относительно других точек. Чтобы этого избежать, можно ввести симметричные распределения $p_{ij} = \frac{p_{i|j} + p_{j|i}}{2n}$ и $q_{ij} = \frac{q_{i|j} + q_{j|i}}{2n}$. Это гарантирует, что $\sum_j p_{ij} > \frac{1}{2n} $, и обеспечивает достаточный вклад каждой точки в итоговое отображение. Ещё одним весомым преимуществом симметричной формы SNE является более простая и проще вычисляемая форма градиента, которая выглядит следующим образом:
\begin{equation}
	\frac{\delta C}{\delta y_i} = 4 \sum_j (p_{ij} - q_{ij})(y_i - y_j)
\end{equation}
На практике симметричная версия SNE работает так же, как и ассиметричная, иногда давая более корректные отображения.

Ещё одной типичной проблемой, возникающей при применении алгоритма является проблема большой размерности исходных данных. При размерности исходных данных $m$ в случае равномерного распределения точек в окрестности некоторой точки $x_i$, при отображении их в двумерное пространство отображения мы столкнемся с проблемой перенаселения: объем $m$-мерной сферы, покрывающей исходные объекты существенно больше двумерной площади вокруг образа точки, и многие точки будут расположены слишком близко по отношению друг к другу, что мешает формированию зазоров между кластерами и зашумляет картину визуализации. Стоит отметить что проблема перенаселения специфична для всех алгоритмов отображения в низкоразмерное пространство, не только для SNE. 

Решение этой проблемы состоит в добавлении равномерного отталкивания, параметризуемого величиной $\rho$, в модель отображения. В этой модели точки не могут находиться ближе, чем $\frac{2 \rho}{n (n + 1)}$, потому что из $n$ точек можно составить $\frac{n (n + 1)}{2}$ пар. И, как результат, если точки $x_i$ и $x_j$ расположены далеко друг от друга, $q_{ij}$ всегда будет больше $p_{ij}$, что гарантирует разделение точек образа. Стоит учесть, что введение отталкивания всегда увеличивает значение целевой функции, и параметр $\rho$ стоит подбирать отдельно от остальных параметров алгоритма.

Ещё с проблемами размерности исходных данных помогает справиться использование для вероятностной модели распределения с более тяжелыми хвостами, чем нормальное. Если в оригинальном высокоразмерном пространстве мы можем моделировать отдаленность точек с помощью нормального распределения, то в низкоразмерном образе мы можем использовать распределение с много более тяжелыми хвостами. Это позволяет моделировать среднее расстояние в оригинальном пространстве большими расстояниями в пространстве-образе, тем самым решая проблему перенаселения более естественным путем. В качестве такого распределения можно применить t-распределение Стьюдента с одной степенью свободы. С его помощью попарные симметричные различия записываются следующим образом:
\begin{equation}
	q_{ij} = \frac{(1 + ||y_i - y_j||^2)^{-1}}{\sum_{k \neq l} (1 + ||y_k - y_l||^2)^{-1}}
\end{equation}

У распределения Стьюдента с одной степенью свободы имеется ещё одна особенность. Его применение удовлетворяет закону обратных квадратов для расстояний $||y_i - y_j||^2$, что придаёт устойчивость к масштабированию результируриющему отображению. Это же обозначает, что большие кластеры точек взаимодействуют между собой так же, как и с единичными точками. Ещё t-распределение Стьюдента представимо как смесь бесконечного количества нормальных распределений с различными дисперсиями, но плотность t-распределения вычисляется быстрее, так как не включает в себя экспоненту. Градиент дивергенции Кульбака-Лейблера между $P$ и моделируемой t-распределением $Q$ задается следующим образом:
\begin{equation}
	\frac{\delta C}{\delta y_i} = 4 \sum_j (p_{ij} - q_{ij})(y_i - y_j)(1 + ||y_k - y_l||^2)^{-1}
\end{equation}

Сравнивая градиенты описанных версий SNE между собой можно сделать следующий вывод: градиент t-SNE выгодно отличается от градиентов SNE и симметричного SNE за счет t-распределения, которое устанавливает большее расстояние между разнородными объектами даже с небольшим попарным различием. Эта особенность t-SNE и повлекла его использование в качестве основного алгоритма визуализации в данной работе.

\subsubsection{Описание алгоритма}

\begin{enumerate}
	\item Построить попарные сходства $p_{i|j}$ с размытостью $Perp$
	\item Построить симметричные попарные сходства: $p_{ij} = \frac{p_{i|j} + p_{j|i}}{2n}$
	\item Подобрать начальное приближение $\mathcal{Y}^{(0)}$
	\item Вычислить низкоразмерные сходства $q_{ij}$
	\item Вычислить градиент $\frac{\delta C}{\delta \mathcal{Y}}$
	\item $\mathcal{Y}^{(t)} = \mathcal{Y}^{(t - 1)} + \eta \frac{\delta C}{\delta \mathcal{Y}} + \alpha(t) (\mathcal{Y}^{(t -1)} + \mathcal{Y}^{(t-2)})$
	\item Повторить шаги 4-6 $n$ раз
\end{enumerate}

\subsection{Требования к валидационному алгоритму}
Валидационный алгоритм должен показывать влияние параметров алгоритма кластеризации на итоговое разбиение. Для достижения цели валидации применяется функция меры различия между разными кластеризациями\cite{wagner2007comparing}. Мера различия $f(\mathcal{C, C^\prime})$ дожна удовлетворять следующим свойствам:
\begin{itemize}
	\item Симметричность: $f(\mathcal{C, C^\prime}) = f(\mathcal{C^\prime, C})$
	\item Неотрицательность: $f(\mathcal{C, C^\prime}) \geq 0$
	\item $f(\mathcal{C, C}) = 0$
	\item $f(\mathcal{C, C^\prime}) = 0 \rightarrow \mathcal{C = C^\prime}$
	\item Неравенство треугольника: $f(\mathcal{C, C^{\prime\prime}}) \leq f(\mathcal{C, C^{\prime}}) + f(\mathcal{C^{\prime}, C^{\prime\prime}})$
\end{itemize}

$f$ удовлетворяющая данным условиям является метрикой на множестве кластеризаций. Можно задать фугкцию сравнения менее строго, не требуя соответствия всем вышеперечисленным свойствам. Так же можно переформулировать свойства для определения не метрики, а функции сходства. Так же существует набор дополнительных ограничений, направленных на независимость $f$ от определённых факторов:
\begin{itemize}
	\item Независимость от размера кластеров
	\item Независимость от числа кластеров
	\item Независимость от расположения кластеров
\end{itemize}

Эти условия исходят из требования независимости от структуры исодных данных.

\subsection{Теоретико-информационные методы} 
\subsubsection{Взаимная информация}
Мера различия между разбиениями играет важную роль в кластеризации данных. Чаще всего применяется следующий подход, называемый внешней валидацией: с помощью меры оценивается схожесть полученной кластеризации с некоторой <<истинной>>. Метрика для внешней валидации выбирается из различных соображений, в частности хорошие результаты показывают методы, основанные на теоретико-информационном подходе\cite{vinh2010information}. Введём понятие энтропии $S$ для текста $T$ с алфавитом $\Sigma$:
\begin{equation}
	S(T) = -\sum_{i \in \Sigma} p_i \log(p_i) 
\end{equation}
$p_i$ здесь это вероятность встретить $i$-тый символ в тексте, а величина $S(T) \cdot T$ -- количество информации, необходимое для представления $T$. Применительно к разбиению множества, $P(i)$ -- вероятность каждого кластера при случайном выборе объекта из кластеризуемой выборки. Для каждого кластера $P(i) = \frac{|C_i|}{n}$. Для каждого разбиения можно ввести величину, называемую энтропией кластеризации $\mathcal{H}(\mathcal{C})$:
\begin{equation}
	\mathcal{H}(\mathcal{C}) = - \sum_{i \in \mathcal{C}} P(i) \log_2P(i)
\end{equation}
Применительно к кластеризации энтропия выражает меру неуверенности об принадлежности случайно выбранного элемента к определённому кластеру. В тривиальном случае единственного кластера для всей выборки мы точно знаем принадлежность для каждого элемента, и энтропия равна 0.

Идея энтропии может быть расширена понятием взаимной информации, описывающей меру уверенности в принадлежности случайно выбранного элемента в одном кластере зная его принадлежность в другой кластеризации на том же множестве элементов. Более формально, взаимная информация между двумя кластеризациями $\mathcal{C}, \mathcal{C}^\prime$ определяется следующим образом:
\begin{equation}
	\mathcal{I}(\mathcal{C}, \mathcal{C}^\prime) = \sum_{i \in \mathcal{C}} \sum_{j \in \mathcal{C}^\prime} P(i,j) \log_2 \frac{P(i,j)}{P(i)P(j)}
\end{equation} 
$P(i,j)$ здесь совместная вероятность того, что элемент принадлежит к кластеру $i$ в $\mathcal{C}$ и к кластеру $j$ в $\mathcal{C}^\prime$:
\begin{equation}
	P(i,j) = \frac{|C_i \cap C_j^\prime|}{n}
\end{equation}

Взаиная информация $\mathcal{I}(\mathcal{C}, \mathcal{C}^\prime)$ является метрикой на пространстве кластеризаций.  Взаимная информация ограничена сверху максимальной энтропией:
\begin{equation}
	\mathcal{I}(\mathcal{C}, \mathcal{C}^\prime) \leq \max(\mathcal{H}(\mathcal{C}),\mathcal{H}(\mathcal{C}^\prime))
\end{equation}
нормализация взаимной информации средним геометрическим или средним арифметическим энтропий может быть полезной. 

Идея нормализации взаимной информации средним геометрическим энтропий кластеризации пришла из задачи наиболее достоверного усреднения результатов кластеризации одного и того же множества разоичными алгоритмами в один без доступа к оригинальным данным и используемым алгоритмам кластеризации. Для её решения применялась максимизация среднего нормированной взаимной информации всех кластеров, при этом нормированная взаимная информация определялась следующим образом:
\begin{equation}
	\mathcal{NMI}_1(\mathcal{C,C}^\prime) = \frac{\mathcal{I(C,C^\prime)}}{\sqrt{\mathcal{H(C),H(C^\prime)}}}
\end{equation}
за идеей нормализации геометрическим средним стоит аналогия с внутренним произведением в Гильбертовом пространстве. 

Существуют попытки решения этой же задачи нормализацией средним арифметическим. Мотивация состоит в удовлетворении следующих условий:
\begin{enumerate}
	\item Консистентность с исходными кластеризациями
	\item Устойчивость к малым изменениям исходных кластеризаций
	\item Быстрая сходимость с ростом количества кластеризаций
\end{enumerate}
Свойство один достигается рассмотрением взаимной информации, нормированной средним арифметическим энтропий:
\begin{equation}
	\mathcal{NMI}_2(\mathcal{C,C}^\prime) = \frac{2\mathcal{I(C,C^\prime)}}{\mathcal{H(C)+H(C^\prime)}}
\end{equation}

Для двух способов нормировки мы имеем $0 \leq \mathcal{NMI} \leq 1$.

\subsubsection{Вариация информации}
Для дифференциации результатов кластеризации предложена и другая метрика, называемая вариацией информации\cite{vinh2010information}:
\begin{equation}
	\mathcal{VI(C,C^\prime)} = \mathcal{H(C) + H(C^\prime)} - 2\mathcal{ I(C,C^\prime)}
\end{equation}

Вариация информации сравнительно нераспространенная мера, анализ которой проведён в соответствующей литературе. Ниже отмечены основные преимущества вариации информации в качестве меры различия кластеризаций:
\begin{itemize}
	\item $\mathcal{VI(C,C^\prime)}$ является метрикой на $\mathcal{P(X)}$
	\item $\mathcal{VI(C,C^\prime)}$ ограничена сверху $\log n$. Если известно число кластеров $K$ и $K \leq \sqrt{n}$, то ограничение можно усилить до $2\log K$. Характер ограничений показывает, что значения метрики лежат в одном диапазоне при достаточно больших $n$ и разных $K$, что позволяет сравнивать кластеризации для различных исходных данных
	\item Малые изменения в кластеризации соответствуют малым изменениям вариации информации
	\item Вариация информации между двумя неравными кластеризациями ограничена снизу величиной $\frac2n$
	\item Вариация информации вычисляется за линейное время относительно размера кластеризуемого множества
\end{itemize}
\subsection{Основные результаты и выводы}
Целью валидации в данной работе является оценка репрезентативности выбранных признаков: насколько они восстанавливают оригинальную структуру в данных. Хорошие результаты в данной задаче достигаются с использованием теоретико-информационных алгоритмов, вводящих меру различия двух разбиений на одном наборе данных. Вариация информации является метрикой и поэтому выглядит предпочтительнее, так как позволяет оценивать близость или дальность двух результатов кластеризации.
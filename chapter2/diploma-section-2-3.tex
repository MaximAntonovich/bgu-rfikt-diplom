\subsection{Иерархическая кластеризация}

\subsubsection{Обоснование алгоритма}

Иерархические алгоритмы кластеризации, называемые так же алгоритмами таксономии, не строят одно разбиение выборки на непересекающиеся кластеры, а возвращают найденную структуру в данных в виде системы вложенных множеств. Как следует из названия, они представляют собой упорядоченную структуру, с определённым кластерным разбиением на каждом уровне. На самом верхнем уровне выделяется один общий кластер, на самом нижнем каждый кластер содержит один объект. В отличие от алгоритмов кластеризации типа k-means и его модификаций, иерархические алгоритмы не требуют знаний о числе кластеров в данных, пользователь может выполнить оценку результата и самостоятельно оценить число кластеров. Однако, при этом иерархические алгоритмы требуют задания метрики различия между группами объектов, чаще всего основанной на попарных различиях точек в двух группах. 

Стратегии разбиения множества на кластеры делятся на две группы: аггломеративные, стартующие с нижнего уровня и последовательно попарно объединяющие кластеры. Таким образом на каждом уровне кластеров становится на один меньше. Выбор пары для слияния на каждом уровне определяется наименьшим значением функции различия. И дивизимные, стартующие с верха иерархии и на каждом уровне разделяющие один кластер на два. Выбор кластера и разбиения производится из принципа максимизации функции различия. Обе группы возвращают $N-1$ уровневую структуру, где $N$- количество объектов в исходном множестве.

Бинарное разделение/объединение может быть представлено бинарным деревом. Узлы дерева представляют собой кластеры, в частности корень дерева представляет оригинальное множество, объединённое в один кластер. $N$ листьев представляют собой единичные кластеры. Каждый узел, не являющийся листом, имеет два потомка, соотвествующих результату разделения в случае дивизимного алгоритма и объединяемым кластерам в случае аггломеративного. 

Все аггломеративные методы и большинство дивизимных следуют свойству монотонности: значение функции различия слияемых или разделяемых кластеров монотонно возрастает с уровнем иерархии. Такое свойство позволяет бинарному дереву быть визуализированным в особой форме, упорядочив узлы по значению фунции различия. Такое отображение называется дендраграммой. Оно хорошо интерпретируемо людьми и является одной из причин высокой популярности иерархических методов. Так же дендрограмма часто используется для визуализации данных сама по себе. Нужно учитывать, что различие функции различия, так же как и небольшие изменения в данных, часто могут кардинально менять дендрогамму. Дендрограмма строится по попарным различиям, и может показать структуру, которой на самом деле в данных нет.

Соответствие реальной структуры в данных  и структуры, показанной дендрограммой, оценивается с помощью коэффициента кофенетической корреляции. Это корреляция между $\frac{N(N-1)}{2}$ значениями функции различия каждой пары объектов и кофенетических коэффициентов $C_{ii^\prime}$, определяемых из дендрограммы. Кофенетический коэффициент $C_{ii^\prime}$ пары наблюдений $(i,i^\prime)$ определяется как значение функции различия между $i$ и $i^\prime$
Кофенетический кэффициент является очень устойчивой мерой. Для любых трех объектов $(i, i^\prime,k)$ выполняется следующее неравенство:
\begin{equation}
	C_{ii^\prime} = \max_k(C_{ik} C_{i^\prime k})
\end{equation}

\subsubsection{Аггломеративные алгоритмы}

Аггломеративные алгоритмы начинают с нижнего уровня иерархии, $N$ единичных кластеров. На каждом шагу они объединяют наиболее близкую пару кластеров. Исходя из этого, должна быть определена мера различия длух групп объектов.

Пусть $G$ и $H$ представляют два кластера. Различие $d(G, H)$ двух кластеров может быть вычислено из попарных различий $d_{ii^\prime}$, где $i$ -- объект, принадлежащий $G$, а $i^\prime$ -- соответственно $H$. Расстояние ближнего соседа -- одна из подобных метрик, вычисляется как наименьшее различие между $i$ и $i^\prime$:
\begin{equation}
	d_{SL}(G,H) = \min_{i \in G, i^\prime \in H} d_{ii^\prime}
\end{equation}

Противоположность -- расстояние дальнего соседа, выбирается наибольшее различие:
\begin{equation}
	d_{CL}(G,H) = \max_{i \in G, i^\prime \in H} d_{ii^\prime}
\end{equation}

Групповое среднее использует усредняет попарное различие между сливаемыми кластерами:
\begin{equation}
	d_{GA}(G, H) = \frac{1}{N_GN_H} \sum_{i \in G}\sum_{i^\prime \in H} d_{ii^\prime}
\end{equation}

Расстояние Уорда вводит в рассмотрение расстояние между центрами с мультипликативной поправкой для достижения монотонности:
\begin{equation}
	d_{W}(G,H) = \frac{|G||H|}{|G|+|H|}d_{\bar{i}\bar{i}^\prime}
\end{equation}

Каждое из этих расстояний удовлетворяет условию монотонности, так как значение функции разбиения на каждом шагу больше, чем на предыдущем, и редуктивности, что позволяет не перебирать все пары сочетаний, ускоряя алгоритм.

В случае когда исходный набор данных представляет собой набор хорошо различимых компактных кластеров, все вышеперечисленные метрики дают схожий результат. В противном случае результаты применения каждой метрики могут сильно различаться. Расстояние ближнего соседа, рассматривая только минимум из попарных различий и не обращая внимания на остальные, имеет тенденцию представлять кластер в виде объединения серии отдельных точек. Это называется цепочечным эффектом и считается недостатком метода. Кластеры, полученные с расстоянием ближайшего соседа нарушают свойство компактности кластеров, так как кластеры с ростом уровня иерархии становятся всё более схожими друг с другом, так как расстояние ближайшего соседа завышает максимум внутрикластерных попарных расстояний. Расстояние дальнего соседа же занижает максимум и производит компактные кластеры. При этом нарушается свойство близости: некоторые кластеры могут быть ближе к соседнему кластеру, чем к некоторым своим точкам. Групповое среднее представляет здесь некоторый компромисс между двумя экстремумами. Кластеры получаются относительно компактными и относительно удалёнными. Однако, вносится зависимость от масштаба попарных различий: если к ним применить монотонное преобразование, результат кластеризации изменится, чего не наблюдается в случае расстояний ближнего и дальнего соседа. Как показывают сравнения разных расстояний, наиболее удачным является расстояние Уорда, широко применяемое в различных областях. 

\subsubsection{Дивизимные алгоритмы}

Дивизимная кластеризация начинает с исходных данных, объединенных в единый кластер, и делит на каждом уровне иерархии один из существующих кластеров. Этот подход значительно менее распространен, чем аггломеративная кластеризация, и впервые был подробно рассмотрен в контексте сжатия информации. Потенциальным преимуществом дивизимных алгоритмов может быть быстрое получение небольшого числа крупных кластеров.

Разделение может осуществляться например комбинаторным методом, таким как К-средних или его модификации. Этот подход вносит зависимость от параметров алгоритма разделения на каждом шагу и может нарушать монотонность значений функции разделения, и поэтому широко не применяется. 

Существует алгоритм разделения, лишенный этих проблем. Он производит следующую процедуру: выбирает точку с наибольшим средним попарным различием с другими точками, которая формирует новый кластер. Далее выбирает те точки, которые в среднем ближе к новообразованному кластеру. Два полученных кластера, образованный и остаток, формируют следующий уровень иерархии вместе с остальными кластерами. Выбор кластера для разделения является отдельной задачей. Можно выбрать кластер с наибольшим максимальным внутренним различием, или кластер, максимизирующий следующий функционал:
\begin{equation}
	\bar{d}_G = \frac{1}{N_G} \sum_{i \in G} \sum_{i^\prime \in G} d_{ii^\prime}
\end{equation}

Разделение продолжается до нижнего уровня, когда все кластеры становятся одиночными, имеющими нулевое внутрикластерное расстояние.

\subsubsection{Описание алгорима}
\begin{enumerate}
	\item Инициализировать начальное множество кластеров
	\item Найти пару ближайших кластеров
	\item Слить выбранные кластеры
	\item Повторить шаги 2-3 на каждом уровне иерархии
\end{enumerate}

\subsection{Спектральная кластеризация}
\subsubsection{Идея алгоритма}
Другим хорошим способом визуализации структуры в данных, кроме дендрограммы, является граф схожести $G = (V,E)$. Вершины графа соответствуют отдельным объектам, ребра соответствуют схожести отдельных объектов, в частности каждому ребру назначается вес $s_{ij}$. Задача кластеризации теперь может быть переформулирована как поиск разбиения графа, оптимального с точки зрения максимизации веса ребер внутри кластеров и минимизации веса ребер между кластерами. Подобная постановка задачи позволяет применить методы теории графов, на чем и основывается спектральная кластеризация.

Введем использумую терминологию. Пусть $G=(V,E)$ ненаправленный взвешенный граф. Веса ребёр образуют матрицу весов графа $W = w_{ij}$, $w_{ij}=0$ соответствует отсутсвию ребра между вершинами $i$ и $j$. Так как $G$ ненаправленный, $w_{ij} = w_{ji}$. Степень вершины определяется как $d_i = \sum_j w_{ij}$. Исходя из определения, вклад в сумму вносят только те вершины, с которыми вершина $i$ соединена непосредственно. Матрица степеней определяется как диагональная матрица из степеней вершин, $D=\text{Diag}(d_1,d_2,...,d_n)$. Для подмножества вершин $A \subset V$ дополнение $\bar{A}$ определяется как $\bar{A} = V \\ A$. Вектор-индикатор $\mathbb{1}_A$ определяется как вектор, $i$-тая компонента которого равна 1, если $i \in A$, и 0 в противоположном случае. Количество вершин в $A$ -- его мощность $|A|$, объем -- сумма вершин: $\text{Vol}(A) = \sum_{i \in A} d_i$. Подмножество $A \subset V$ называется соединённым, если между любыми двумя вершинами существует путь, все вершины которого тоже принадлежат $A$. $A$ называется связанным компонентом, если не существует ребер между вершинами $A$ и $\bar{A}$. Множества $A_1,..,A_k$ называются разбиением графа, если $\bigcup_i A_i = G$, и $A_i \cap A_j = \emptyset \forall i,j$.

\subsubsection{Различные способы построения графа схожести}
Существует несколько способов построения графа схожести из данных. Каждый способ построения стремится сохранить локальную структуру данных, и поэтому большиинство способов возвращают разреженную матрицу, что может быть удобно в вычислительном плане.

\textbf{Граф $\varepsilon$-соседства}: Каждая вершина соединяется ребром с теми, которые лежат в $\varepsilon$-окрестности точки, соответствующей данной вершине. Так как сходства при данном способе построения фиксированы, данный способ построения не присваевает веса ребрам.

\textbf{Граф $k$-ближайших соседей}: При данном способе построения вершины $i$ и $j$ соединяются ребром, если $j$ входит в число $k$ ближайших соседей $i$. Как следует из способа построения, граф получается направленным. Существует способ получить ненаправленный граф, в этом случае проводят ребро между $i$ и $j$ в том случае, если $i$ входит в число ближайших соседей $j$ и одновременно с этим $j$ входит в число ближайших соседей $i$. В любом случае, проведённым ребрам добавляется вес $w_{ij}$.

\textbf{Полносвязный граф}: При данном способе построения все вершины соединяются между собой и взвешиваются. Для придания большего веса ближайшим соседям сходство часто подвергается радиально-базисному ядру: $w_{ij}=\exp(-\frac{||w_i - w_j||^2}{\sigma^2})$. 

Вск вышеперечисленные способы построения часто применяются в приложениях. 

\subsubsection{Лапласиан графа}
Основным приемом спектрального кластеризации является анализ лапласиана графа. Ненормированый лапласиан графа это матрица, определяемая следующим образом:
\begin{equation}
	L = D - W
\end{equation}

Свойства ненормированного лапласиана:
\begin{enumerate}
	\item Лапласиан является квадратичной формой, для каждого вектора $\mathit{f}$:
	\begin{equation}
		f^T L f = \frac12 \sum_{i,j} w_{ij}(f_i - f_j)^2
	\end{equation} 
	\item Лапласиан графа симметричен и положительно определён
	\item Наименьшее собственное значение лапласиана 0, ему соответствует единичный собственный вектор $\mathbb{1}$
	\item Лапласиан имеет $n$ вещественных неотрицательных собственных значений: $0=\lambda_1 \leq \lambda_2 \leq ... \leq \lambda_n$
\end{enumerate}

Отметим, что ненормализованный лапласиан не зависит от диагональных элементов матрицы $W$. В частности, каждая матрица $U$, совпадающая с $W$ во всех недиогональных элементах, приводит к тому же ненормализованному лапласиану, что исключает влияние петель в графе. 

Собственные вектора лапласиана имеют много полезных свойств. Некоторые из них важны для решения задачи кластеризации. Пусть $G$ -- ненаправленный граф с неотрицательными весами. Тогда кратность собственного значения 0 в ненормированном лапласиане $L$ равна числу связанных компонент $A_1,..,A_k$ в графе. Пространство собственных векторов собственного значения 0 покрывается векторами-индикаторами $\mathbb{1}_{A_1},..,\mathbb{1}_{A_n}$ соответствующих компонент. Это даёт четкий критерий числа кластеров.

В литературе определяется два типа норированных лапласианов, связанных друг с другом. Одна из них$\mathcal{L}_{sym}$, является симметричной и  использутеся в приложениях, другая же $\mathcal{L}_{rw}$ возникает в задачах исследования случайных блужданий по графу. Они определяются следующим образом:
\begin{equation}
\begin{array}{l}
	\mathcal{L}_{sym} = D^{-\frac12} L D^{-\frac12} = I - D^{-\frac12} W D^{-\frac12} \\
	\mathcal{L}_{rw} = D^{-1} L = I - D^{-1} W
\end{array}
\end{equation}

Обе эти матрицы обладают следующими свойствами:
\begin{enumerate}
	\item Для любого вектора $\mathit{f} \in \mathbb{R}^n$ мы имеем:
	\begin{equation}
		\mathit{f}^T \mathcal{L}_{sym} \mathit{f} = \frac12 \sum_{i,j} w_{ij}\left(\frac{\mathit{f}_i}{\sqrt{d_i}} - \frac{\mathit{f}_j}{\sqrt{d_j}}\right)^2
	\end{equation}
	\item $\lambda$ является собственным значением $\mathcal{L}_{rw}$ с собственным вектором $v$ в том и только в том случае, когда $\lambda$ является собственным значением $\mathcal{L}_{sym}$ c собственным вектором $w = D^{\frac12} v$
	\item $\lambda$ является собственным значением $\mathcal{L}_{rw}$ с собственным вектором $v$ в том и только в том случае, когда $\lambda$ и $w$ являются решением обобщенной задачи поиска собственных значений: $L v = \lambda D v$
	\item 0 является собственным значением $\mathcal{L}_{rw}$ c собственным вектором $\mathbb{1}$ и собственным значением $\mathcal{L}_{sym}$ с собственным вектором $D^{\frac12} \mathbb{1}$
	\item $\mathcal{L}_{sym}$ и $\mathcal{L}_{rw}$ положительно полуопределённые с $n$ неотрицательными действительными собственными значениями $0=\lambda_1 \leq \lambda_2 \leq ... \leq \lambda_n$
\end{enumerate}

Так же, как и в случае с ненормализованным лапласианом, кратность нулевого собственного значения имеет отношение к числу кластеров. Для $\mathcal{L}_{sym}$ собственное подпростанство значения 0 будет натянуто на векторы $\mathbb{1}_{A_i}$, а для $\mathcal{L}_{rw}$ на векторы $D^{\frac12} \mathbb{1}_{A_i}$.

\subsubsection{Построение графа схожести}
Выбор способа построения графа зависит от многих факторов и является нелёгкой задачей. Наилучшим средством поиска способа построения графа является экспертный анализ кластеризуемых данных. Такие параметры как форма образуемых кластеров, плотность точек, количество и характер выбросов по-разному обрабатываются при различных способах построения и влияют на итоговый результат. Нельзя игнорировать и происхождение данных. В случае выбора графа $\varepsilon$-соседства очень большое значение имеет выбор $\varepsilon$. В случае разных плотностей в кластерах часто бывает невозможно выбрать значение, удовлетворительно разбивающее исхожное множество. Граф $k$-ближайших соседей корректно обрабатывает кластеры разной плотности, но может не распознать сложный по форме кластер, разбив его на несколько примитивов. Построение графа $k$-взаимных соседей выглядит компромиссом между этими двумя альтернативами. С одной стороны регионы с различной плотностью точек не объединяются, с другой -- тенденция к потере формы кластеров сохраняется. Построение полносвязного графа дает больше информации о локальной струтуре, но матрица перестает быть разреженной и её обработка может быть вычислительно затратной. 

В общем, результат спектральной кластеризации чувствителен к итоговой матрице и выбору параметров её построения. К сожалению, не существует простой процедуры настройки эффектов выбранного способа построения.

\subsubsection{Выбор числа кластеров}
Выбор числа кластеров является насущной проблемой для любого алгоритма кластеризации. Для спектральной кластеризации решение такой проблемы представляет собой эвристика над собственными значениями всех трех матриц лапласиана: ненормализованной и двумя нормализованными. Цель этой эвристики найти такое $k$, что $\lambda_1,..,\lambda_k$ относительно малые, так как $\lambda_{k+1}$ много больше $\lambda_k$. Обоснование этой эвристики лежит в описанном выше свойстве матриц лапласиана, в идеальном случае кратность нуля равняется количеству кластеров. 

\subsubsection{Описание алгоритма}
\begin{enumerate}
	\item Выбрать способ построения графа и используемый лапласиан
	\item Построить граф и вычислить его лапласиан
	\item Решить задачу поиска собственных векторов и собственных значений для лапласиана
	\item Отобрать первые $k$ собственных векторов и составить из них матрицу $n \times k$
	\item Кластеризовать строки матрицы алгоритмом k-means
\end{enumerate}
\subsection{Основные результаты и выводы}
В качестве алгоритмов кластеризации были выбраны алгоритмы иерархической и спектральной кластеризации. Преимущества алгоритма иерархической кластеризации: воспроизводимость, интерпретируемость, визуальное представление с помощью дендрограммы, гибкость, реализуемая заданием метрики и правила объединения или деления кластеров. Преимущества спектральной кластеризации: спектральная теория графов в качестве обоснования, корректное выделение в кластеры сложных структур данных. Так же к достоинствам этих алгоритмов можно отнести возможность выбора оптимального числа кластеров после завершения работы алгоритма, а не заранее в качестве параметра.
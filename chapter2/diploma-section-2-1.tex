\subsection{Laplacian Score}

\subsubsection{Идея алгоритма}

В основе алгоритма вычисления Laplacian Score лежит наблюдение, что два объекта возможно принадлежат к одному и тому же классу,
если они лежат рядом друг с другом. Исходя из этого, признаки объектов можно ранжировать по их способности передавать близость соседних точек. Алгоритм предлагает технику Laplacian Eigenmaps для описания этой способности\cite{he2005laplacian}. 

\subsection{Laplacian Eigenmaps}

Техника Laplacian Eigenmaps пришла из спектральной теории графов. Пусть дано $n$ точек в $\mathbb{R}^l$. Построим из них направленный
взвешенный граф по следующим правилам:
\begin{enumerate}
	\item Каждой точке соответствует вершина графа.
	\item Проведем дугу из вершины, соответствуещей $x_i$ в вершину, соответствующую $x_j$, если $x_j$ входит в число $k$ ближайших соседей
	$x_i$.
	\item Назначим ребру вес $\mathbf{f}(||x_i - x_j||)$, где $\mathbf{f}$ -- монотонная возрастающая функция.
\end{enumerate}
Пусть $S$ - матрица связности полученного графа, $D$  - диагональная матрица, у которой $D_{ii} = \sum_j S_{ij}$, $L$ -- лапласиан графа, $L = D-S$. Из спектральной теории графов, лапласиан способен отображать структуру данных\cite{belkin2003laplacian}.

\subsubsection{Описание алгоритма}

\begin{enumerate}
	\item Построить граф $k$-ближайших соседей из исходных данных. 
	\item Вычислить лапласиан полученного графа.
	\item Вычислить приведённые векторы признаков: 

	Пусть $\mathbf{f}_r$ вектор $r$-того признака из исходных данных. Тогда $\widetilde{\mathbf{f}_r} = \mathbf{f}_r - \frac{\mathbf{f}_r^T D \mathbf{1}}{\mathbf{1}^T D \mathbf{1}} \mathbf{1}$.
	\item Вычислить Laplasian score для каждого признака.
	\[L_r = \frac{\widetilde{\mathbf{f}_r}^T L \widetilde{\mathbf{f}_r}}{\widetilde{\mathbf{f}_r}^T D \widetilde{\mathbf{f}_r}}\]
\end{enumerate}

\subsubsection{Обоснование алгоритма}

Напомним, алгоритм работает со взвешенным графом $G$, веса описываются матрицей $S_{ij}$, "хорошими" признаками считаются те, которые сохраняют близость исходных точек. Подходящим критерием для отбора признаков является оптимум следующей функции\cite{he2005laplacian}:

\begin{equation} \label{goal_function:1}
	L_r=\frac{\sum_{ij}(f_{ri} - f_{rj})^2 S_{ij}}{Var(\mathbf{f}_r)}
\end{equation} 
где $Var(\mathbf{f}_r)$ -- оценочная дисперсия $r$-того признака. Так как минимуму $\sum_{ij}(f_{ri} - f_{rj})^2 S_{ij}$ соответствуют более близкие значения соответствующих признаков у соседних объектов, "хорошему" признаку будут соответствовать наименьшие значения $L_r$. Отбор этих признаков бцдет соответствовать сохранению структуры графа. Проведем некоторые преобразования числителя:

\begin{equation}
	\begin{array}{l}
	\sum_{ij} (f_{ri} - f_{rj})^2 S_{ij} = \sum_{ij} (f_{ri}^2 + f_{rj}^2 -2 f_{ri} f_{rj}) S_{ij} = \\
	= \sum_{ij} 2 f_{ri}^2 S_{ij} - 2 f_{ri}  S_{ij} f_{rj} = 2 \mathbf{f}_r^T D \mathbf{f}_r - 2 \mathbf{f}_r^T S \mathbf{f}_r = 2 \mathbf{f}_r^T L \mathbf{f}_r
	\end{array}
\end{equation}
Далее найдем дисперсию каждого признака:

\begin{equation}
	\begin{array}{l}
	\mu_r = \sum_i \mathbf{f}_{ri} D_{ii} = \frac{\mathbf{f}_r^T D \mathbf{1}}{\mathbf{1}^T D \mathbf{1}} \\
	Var(\mathbf{f}_r) = \sum_i (\mathbf{f}_{ri} - \mu_r)^2 D_{ii} = \sum_i (\mathbf{f}_r - \frac{\mathbf{f}_r^T D \mathbf{1}}{\mathbf{1}^T D \mathbf{1}})^2 D_{ii} = \widetilde{\mathbf{f}}^T D \widetilde{\mathbf{f}}
	\end{array}
\end{equation}
Отсюда, с учетом $\mathbf{f}^T D \mathbf{f} = \widetilde{\mathbf{f}}^T D \widetilde{\mathbf{f}}$ получается итоговая формула расчёта критерия.

\subsection{SPEC Score}
\subsubsection{Идея алгоритма}
Алгоритм SPEC Score обобщает Laplacian Score и развивает подход с применением спектральной теории графов для отбора признаков c оптимизацией целевой переменной \ref{goal_function:1}. Авторы SPEC отмечают два побочных фактора, влияющих на величину переменной, а именно норма вектора признаков $\mathbf{f}_i $ и норма лапласиана графа $L$. В реализации алгоритма оба этих фактора убираются с помощью нормализации\cite{zhao2007spectral}:

\begin{equation}
	\begin{array}{l}
	\mathbf{f}_i^T L \mathbf{f}_i = \mathbf{f}_i^T D^\frac12 \mathcal{L} D^\frac12 \mathbf{f}_i = (D^\frac12 \mathbf{f}_i)^T \mathcal{L} (D^\frac12 \mathbf{f}_i) = \widetilde{\mathbf{f}}_i^T \mathcal{L} \widetilde{\mathbf{f}}_i \\
	\widehat{\mathbf{f}}_i = \frac{\widetilde{\mathbf{f}}_i}{||\widetilde{\mathbf{f}}_i||} \\
	\varphi_1(\mathbf{f}_i) = \widehat{\mathbf{f}}_i^T \mathcal{L} \widehat{\mathbf{f}}_i
	\end{array}
\end{equation}
$\varphi_1$ и представляет собой SPEC score. Кроме этого авторы алгоритма рассматривают несколько альтернативных вариантов расчета признака. Рассмотрим разложение нормализованного лапласиана $\mathcal{L}$ на $(\lambda_i, \xi_i)$, $\lambda_i$ -- $i$-тое собственное значение, $\xi_i$ -- $i$-тый собственный вектор, при этом $\lambda_0 \leq \lambda_1 \leq ... \leq \lambda_{n-1}$. Заметим, что $\lambda_0 = 0$ является собственным значением с собственным вектором $\xi_0 = D^\frac12 \mathbf{e}$. Используя расложение лапласиана можно переписать выражение для целевой переменной:

\begin{equation}
	\begin{array}{l}
		\alpha_j = \cos(\widehat{\mathbf{f}}_i, \xi_j)\\
		\varphi_1(\mathbf{f}_i) = \widehat{\mathbf{f}}_i^T \mathcal{L} \widehat{\mathbf{f}}_i = \widehat{\mathbf{f}}_i^T U \Sigma U^T \widehat{\mathbf{f}}_i = \\
		= (\alpha_0, \alpha_1, ... , \alpha_{n-1}) \Sigma (\alpha_0, \alpha_1, ... , \alpha_{n-1})^T = \sum_{i=0}^{n-1} \alpha_i^2 \lambda_i
	\end{array} 
\end{equation}
$\alpha_i$ вырыжают схожесть между вектором признака и собственными векторами нормированного лапласиана. Как следует из спектральной теории графов, собсвенные значения $\mathcal{L}$ определяют разделяемость компонент графа, а собственные векторы соответствуют оценкам классовой принадлежности. Так как $\lambda_0 = 0$, оно не несет информации о компонентах и имеет смысл исключить соответствующий собственный вектор $\xi_0$, что приводит к следующему выражению для значения признака:

\begin{equation}
	\varphi_2(\mathbf{f}_i) = \frac{\widehat{\mathbf{f}_i}^T \mathcal{L} \widehat{\mathbf{f}_i}}{1 - \widehat{\mathbf{f}_i}^T \xi_0}
\end{equation} 
Меньшие значения $\varphi_2$ говорят о близости вектора признака к собственным векторам с меньшими собственными значениями, что соответствует лучшей разделяемости компонент и сохранению структуры графа. Если число кластеров известно заранее, то в соответствии со спектральной теорией графов можно рассмотривать $k$ векторов с наибольшими собственными значениями, где $k$ -- число кластеров. Это приводит к следующему выражению для значения признака:

\begin{equation}
	\varphi_3(\mathbf{f}_i) = \sum_{j=1}^{k}(2 - \lambda_j)\alpha_j^2
\end{equation}

Исходя из определения $\varphi_3$, большие значения признака соответствуют лучшей отделяемости, кроме этого $\varphi_3$ всегда больше нуля, так как $\lambda_i$ лежат на отрезке $[0, 2]$. Из-за фокуса на собственных векторах с наименьшими собственными значениями, значения признака в третьем случае имеют меньшую тенденцию к зашумлению. 

\subsubsection{Описание алгоритма}
\begin{enumerate}
	\item Построить граф $k$-ближайших соседей из данных
	\item Вычислить $D, W, L, \mathcal{L}$ графа
	\item Вычислить $\widehat{\mathbf{f}_i}$ и $\varphi$ для каждого признака
	\item Ранжировать признаки по возрастанию для $\varphi_1$ и $\varphi_{2}$ и по убыванию для $\varphi_3$
\end{enumerate}

\subsection{Multicluster feature score}

\subsubsection{Идея алгоритма}
	В основе следующего алгоритма отбора признаков лежит два предложения:
	\begin{itemize}
		\item Отобранные признаки должны отражать наилучшим образом кластерную структуру в данных. При этом кластеры в данных могут быть негауссовыми.
		\item Признаки, хорошо подходящие для разделения одной пары кластеров могут плохо подходить для разделения другой пары. Итоговый набор признаков должен покрывать все кластеры в данных.
	\end{itemize}

	Для соответствия первому предложения используется спектральная теория графов, с рассмотрением лапласиана графа ближайших соседей. Для соответствия второму пункту необходимо знать число кластеров в исходных данных, после чего можно решать несколько задач оптимизации для отбора признаков, одну для каждого класса\cite{cai2010unsupervised}. После чего можно объединить полученные решения. Рассмотри этот подход подробнее.

	Спектральный анализ графа включает в себя вычисление матрицы весов графа ближайших соседей $W$ и матрицы степеней вершин $D$. Можно рассмотреть обобщенную задачу нахождения собственных векторов лапласиана:
	\begin{equation}
		W \mathbf{y} = \lambda D \mathbf{y}
	\end{equation}

	 В отличие от классической постановки, тут каждому собственному значению удовлетворяет множество собственных векторов. Пусть 
	 $Y = [\mathbf{y}_1, \mathbf{y}_2,...,\mathbf{y}_k]$ -- $k$ 
	 собственных векторов, соответствующих наименьшему собственному значению. Каждая строка $Y$ соответствует отображению соответствующего объекта в пространство меньшей размерности. 

	 Имея отображение в пространство более низкой размерности, мы можем ранжировать оригинальные признаки по важности, исследуя вклад каждого признака в распределение кластеров в исследуемом отображении. Для этого составим следующую задачу оптимизации для каждого $\mathbf{y}_k$:
	 \begin{equation}
	 	\min_{\mathbf{a}_k} ||\mathbf{y}_k - X^T \mathbf{a}_k||^2 + \beta |\mathbf{a}_k|
	 \end{equation}

	 $\mathbf{a}_k$ состоит из коэффициентов, с которыми разные признаки аппроксимируют $\mathbf{y}_k$. Из-за природы L1-регуляризации с увеличением $\beta$ малые компоненты $\mathbf{a}_k$ стремятся к 0, что позволяет устанавливать порог важности для признаков. Решением задач оптимизации будут $k$ разреженных векторов $\mathbf{a}_k$. Для каждого признака $j$ мы можем определить multicluster feature score $\text{MCFS}(j)$ как:
	 \begin{equation}
	 	\text{MCFS}(j) = \max_k |a_{k,j}|
	 \end{equation}

	 где $a_{k,j}$ $j$-тый элемент $\mathbf{a}_k$. После можно отсортировать ранжированные признаки по убыванию.

	 \subsubsection{Описание алгоритма}
	 \begin{enumerate}
	 	\item Построить граф ближайших соседей из данных
	 	\item Решить обобщенную задачу нахожения собственных векторов лапласиана
	 	\item Решить $K$ регуляризованных задач оптимизации
	 	\item Вычислить MCFS для каждого признака
	 	\item Отсортировать ранжированные прзнаки
	 \end{enumerate}
\subsection{Nonnegative discriminative feature score}
\subsubsection{Идея алгоритма}
Идея авторов алгоритма состоит в одновременной кластеризации и ранжировании признаков, причем в спектральной кластеризации используется модифицированная целевая функция с добавленным условием на неотрицательность значений. Обосновывается это большей близости результатов кластеризации к ожидаемым. Отбор признаков обеспечивается условием разреженности матрицы признаков по строкам, которое формируется в терминах минимизации $\mathcal{l}_{2,1}$-нормы матрицы\cite{yang2011l2}. Для любой матрицы $A \in \mathbb{R}^{n \times k}$ $\mathcal{l}_{2,1}$-норма определяется как 
\begin{equation}
	||A||_{2,1} = \sum_{i=1}^r \sqrt{\sum_{j=1}^k A_{ij}^2}
\end{equation} 

Выполнение условий и минимизация нормы матрицы достигаются оптимизационным алгоритмом, описанным далее. Построим целевую функцию для оптимизации. Пусть $X = [\mathbf{x}_1,...,\mathbf{x}_n]$ -- матрица исходных данных. Определим $Y$ как $[\mathbf{y}_1,...\mathbf{y}_n]$, где $\mathbf{y}_i \in {0,1}^{c\times n}$ -- индикатор принадлежности $i$-того объекта к некоторому кластеру. Определим нормированный индикатор: $F=Y(Y^T Y)^\frac12$. Нормированный кластерный индикатор удобен тем, что по определению является ортогональной матрицей:
\begin{equation}
	F^T F = (Y^T Y)^\frac12 Y^T Y (Y^T Y)^\frac12 = I
\end{equation}

Построим целевую функцию для оптимизации, при этом будем одновременно будем искать и оптимальное разбиение исходных данных $F$, и отобранные признаки $W$. 
\begin{equation}
	\mathcal{J}(F) = \alpha(||X^T W - F||^2 + \beta||W||_{2,1})
\end{equation}
Предложенная целевая функция работает и в случае с зашумленными и скореллированными признаками. Определим $\mathbf{w}_j$ как $j$-тую строку $W$. Сочетание минимизации целевой функции и $\mathcal{l}_{2,1}$регуляризации дает $W$ способность описывать корреляцию между кластерным разбиением и сочетанием признаков, а именно стремление $\mathbf{w}_j$ к 0 обозначает меньшее влияние $j$-признака на нормированную матрицу кластерной принадлежности $F$.

$\mathcal{J}(F)$ в предложенном виде не учитывает явно локальность в исходных данных, тогда как она играет ключевую роль в ранжировании признаков, что показано в вышеописанных алгоритмах. Модифицируем целевую функцию, добавив в неё слагаемое, зависящее от лапласиана графа ближайших соседей. 
\begin{equation}
	\frac12 \sum_{i,j=1}^n S_{i,j} ||\frac{\mathbf{f}_i}{A_{i,j}}  - \frac{\mathbf{f}_j}{A_{i,j}}||^2_2 = \text{Tr}[F^T L F]
\end{equation}

При достижении минимума $\mathcal{J}$ некоторые элементы $F$ могут окзаться отрицательными, что не удовлетворяет свойствам $F$  и приводит к нарушению кластерной структуры при восстановлении $Y$. Чтобы избежать этого, неотрицательность и ортогональность $F$ тоже явно вводятся в целевую функцию. Её окончательное определение выглядит следующим образом:
\begin{equation}
	\begin{array}{r}
		\mathcal{J}(F) = \text{Tr}[F^T L F] + \alpha(||X^T W - F||^2 + \beta||W||_{2,1}) \\
		\text{s. t. } F^T F = I, F \geq 0
	\end{array}
\end{equation}

Существует вариант алгоритма с более простой целевой функцией\cite{li2012unsupervised}, 

Для решения задачи оптимизации построим следующий алгоритм. Перепишем целевую функцию в следующем виде, явно добавив ограничения в качестве слагаемых:
\begin{equation}
	\mathcal{I}(F, W) = \text{Tr}[F^T L F] + \alpha(||X^T W - F||^2 + \beta ||W||^2_{2,1}) + \frac{\gamma}{2} ||F^T F - I||^2
\end{equation}

В экстремуме $\frac{ \partial\mathcal{I}(F, W)}{\partial W} = 0$.

\begin{equation}
	\begin{array}{r}
	\frac{\partial{\mathcal{I}(F, W)}}{\partial{W}} = 2 \alpha (X (X^T W - F) + \beta D W) = 0 \rightarrow \\
\rightarrow W = (XX^T + \beta D)^{-1} X F
	\end{array} 
\end{equation}

Здесь $D$ -- диагональная матрица, $D_{ii} = \frac{1}{2 ||\mathbf{w}_i||^2}$. Подставим полученное выражение для $W$ в целевую функцию:
\begin{equation}
	\min_{F, W} \text{Tr} [F^T M F ] + \frac{\gamma}2||F^T F - I||^2
\end{equation}
где $M=L + \alpha(I - X^T(X X^T + \beta D)^{-1} X)$. Постоим теперь функцию Лагранжа для оптимизации:
\begin{equation}
	\text{Tr}[F^T M F] + \frac\gamma2||F^T F - I||^2_F + \text{Tr}(\Phi F^T)
\end{equation}

Приравнивая её производную к нулю, мы получим выражение для обновления $F$ на каждом шагу.
\begin{equation}
	F_{ij} \leftarrow F_{ij} \frac{(\gamma F)_{ij}}{(MF + \gamma F F^T F)_{ij}}
\end{equation}

Остается нормализовать $F$ для достижения условия $(F F^T)_{ii} = 1 $

\subsubsection{Описание алгоритма}
\begin{enumerate}
	\item Построить граф ближайших соседей и вычислить лапласиан $L$
	\item Инициализировать матрицы $F$ и $D$
	\item Вычислить $M$: $M^t = L + \alpha(I - X^T(X X^T + \beta D)^{-1} X)$
	\item Обновить $F$: $F_{ij}^{t+1} = F_{ij}^t \frac{(\gamma F^t)_{ij}}{(MF^t + \gamma F^t F^{tT} F^t)_{ij}}$
	\item Обновить $W$: $ W^{t+1} = (XX^T + \beta D^t)^{-1} X F^t$
	\item Обновить $D$: $D_{ii}^{t+1} = \frac{1}{2 ||\mathbf{w}_i^T||^2}$
	\item t = t+1
	\item Повторять 3-7 до достижения условия сходимости
\end{enumerate}

\subsection{Генетический алгоритм}
\subsubsection{Идея алгоритма}
Популярным решением задач комбинаторной оптимизации являются генетические алгоритмы. Так как задача отбора признаков может быть переформулирована в терминах комбинаторной оптимизации, генетический алгоритм может быть использован для её решения\cite{yang2010nature}.

Генетический алгоритм ищет оптимальное решение на дискретном пространстве $\mathcal{X}$. Под оптимальным решением здесь подразумевается оптимум некоторой функции $f : \mathcal{X} \rightarrow \mathbb{R}$.
\begin{equation}
	\text{argmin}_{x \in \mathcal{X}} f
\end{equation}
Здесь $x$ -- вектор переменных решения, $f$ -- целевая функция. Задача поиска минимума $f$ на $\mathcal{X}$ называется задачей дискретной комбинаторной оптимизации. 

Одной из интересных особенностей генетического алгоритма, является интерпретация такого рода задачи как моделирование процесса эволюции. В этом случае вектор $x$ представляет собой так называемый <<генотип>>, кодированное представление объекта, называемого <<фенотипом>>. Исходя из этих представлений, генотип представляет собой множество строк фиксированной длины с конечным алфавитом, а каждому объекту из фенотипа соответствует одна такая строка. При этом, если рассмотреть множество строк определённого выше типа, то далеко не каждая из них соответствует возможному объекту, из этого возникает проблема кодирования и соответствия генотип-фенотип. Способ кодирования является одним из параметров алгоритма\cite{gendreau2010handbook}.

Продолжая биологическую аналогию, в генетическом алгоритме вводится понятие популяции -- группы особей, имеющих общие черты в геноме. Особь характеризуется хромосомой, состоящей из генов. Генетический материал переносится внутри популяции двумя операциями, называемыми кроссовером и мутацией. Кроссовером называется операция над двумя хромосомами, при которой соотвестсвующие гены одной хромосомы меняются с генами другой хромосомы. Биологическая аналогия здесь это скрещивание. Мутацией называется случайная замена генов в хромосоме. Эволюционный процесс моделируется до стабилизации результата.
\subsubsection{Описание алгоритма}
\begin{enumerate}
	\item Инициализировать популяцию случайными хромосомами
	\item Разбить популяцию на пары и провести скрещивание
	\item Отобрать хромосомы и провести мутацию
	\item Сформировать следующее поколение популяции
	\item Вычислить целевую функцию
	\item Повторить шаги 2-5 до достижения условия сходимости
\end{enumerate}
\subsection{Основные результаты и выводы}
Важное место в отборе признаков в кластеризации занимает анализ признаков средствами спектральной теории графов. Каждый из описанных алогоритмов, использующий спектральную теорию графов, вносит свои факторы в процесс отбора признаков, такие как использование ядер для функции показателя важности признака (SPEC), оценки важности признаков отдельно для каждого кластера и регуляризации (MCFS), использование неотрицательной матричной факторизации и регуляризации(NDFS и UDFS). Важность каждого из факторов требует отдельной оценки на конкретном наборе данных.

Альтернативу отбору признаков с помощью спектральной теории графов составляет генетический алгоритм, решающий задачу комбинатрной оптимизации. Его особенность состоит в игнорировании зависимостей в данных. Влияние данной особенности тоже требует практической оценки.